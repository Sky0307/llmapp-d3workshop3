name: Promptfoo Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  promptfoo-eval:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build backend image
        run: docker build -t llm-backend ./llm-multiroute

      - name: Start backend container
        run: |
          docker run -d \
            --name llm-backend \
            -p 8080:8080 \
            -e OLLAMA_API_KEY=${{ secrets.OLLAMA_API_KEY }} \
            -e OLLAMA_BASE_URL=${{ secrets.OLLAMA_BASE_URL || 'https://ollama.com' }} \
            -e OLLAMA_MODEL_CLASSIFY=gemma3:4b \
            -e OLLAMA_MODEL_SENTIMENT=ministral-3:3b \
            -e OLLAMA_MODEL_SUMMARIZE=ministral-3:8b \
            -e OLLAMA_MODEL_INTENT=gemma3:12b \
            llm-backend

      - name: Wait for backend to be ready
        run: |
          echo "Waiting for backend to start..."
          for i in $(seq 1 30); do
            if curl -s http://localhost:8080/swagger-ui.html > /dev/null 2>&1; then
              echo "Backend is ready!"
              exit 0
            fi
            echo "Attempt $i/30 - waiting..."
            sleep 2
          done
          echo "Backend failed to start"
          docker logs llm-backend
          exit 1

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Run promptfoo tests
        working-directory: ./promptfoo-tests
        run: |
          npx promptfoo@latest eval -c classify.yaml --no-cache
          npx promptfoo@latest eval -c sentiment.yaml --no-cache
          npx promptfoo@latest eval -c summarize.yaml --no-cache
          npx promptfoo@latest eval -c intent.yaml --no-cache

      - name: Print backend logs on failure
        if: failure()
        run: docker logs llm-backend

      - name: Stop backend container
        if: always()
        run: docker stop llm-backend && docker rm llm-backend